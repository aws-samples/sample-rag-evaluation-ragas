{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve RAG evaluation with Ragas and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites section\n",
    "\n",
    "Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install necessary packages\n",
    "%pip install --force-reinstall -q -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets restart the kernel to make sure python packages are installed and imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will use the Sagemaker FAQ, consisting of 170 questions and answers, to build and evaluate a RAG (Retrieval-Augmented Generation) application using Ragas and LlamaIndex.\n",
    "\n",
    "We'll use Langchain, a Python framework for developing applications powered by language models, to create our RAG application. Langchain simplifies the creation and deployment of RAG applications.\n",
    "\n",
    "We use Amazon Bedrock wih Claude 3 sonnet as the underlying language model for our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "# getting boto3 clients for required AWS services\n",
    "sts_client = boto3.client('sts')\n",
    "iam_client = boto3.client('iam')\n",
    "s3_client = boto3.client('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the source web url, model Ids \n",
    "Before you start, setup a Knowledge base manually and put the ID in the utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#URL to fetch the document\n",
    "SAGEMAKER_URL=\"https://aws.amazon.com/sagemaker/faqs/\"\n",
    "\n",
    "#Bedrock parameters\n",
    "EMBEDDING_MODEL=\"amazon.titan-embed-text-v2:0\"\n",
    "BEDROCK_MODEL_ID=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=EMBEDDING_MODEL,client=bedrock_client)\n",
    "\n",
    "model_kwargs = {\"temperature\": 0, \"top_k\": 250, \"top_p\": 1,\"stop_sequences\": [\"\\n\\nHuman:\"]}\n",
    "\n",
    "llm_bedrock = ChatBedrock(model_id=BEDROCK_MODEL_ID,model_kwargs=model_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split FAQ document and chuncks\n",
    "\n",
    "\n",
    "1. **Website Scraping and Data Loading**: Load the FAQ data using the WebBaseLoader class from Langchain to parse the FAQ website and load it into a Langchain documents object.\n",
    "2. **Document Splitting**: Split the document into chunks of 2000 words with an overlap of 200 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split_document_from_url, get_bedrock_retriever\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "text_chunks = split_document_from_url(SAGEMAKER_URL, chunck_size= 2000,  chunk_overlap=100)\n",
    "retriever_db= get_bedrock_retriever(text_chunks, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a retrival chain using Lnagchain and Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise and short. \"\n",
    "    \"Context: {context}\"\n",
    "    )\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm_bedrock, prompt_template)\n",
    "\n",
    "chain = create_retrieval_chain(retriever_db, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon SageMaker?\"\n",
    "result=chain.invoke({\"input\": query})['answer']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG with Ragas\n",
    "\n",
    "\n",
    "In this example we use the Ragas library to evaluate the RAG application with 3 metrics:\n",
    "\n",
    "\n",
    "1. **Faithfulness**: This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. This is useful for measuring if the response was hallucinated.\n",
    "\n",
    "2. **Answer relevancy**: The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. This is useful for measuring if the query was actually answered by the response.\n",
    "\n",
    "3. **Answer correctness**: This metric measures the correctness of the generated answer. It is calculated from the groundtruth answer and the question.\n",
    "\n",
    "\n",
    "Note : Ragas offer a wide range of metrics to evaluate RAG applications. For more information, please refer to the [Ragas documentation](https://docs.ragas.io/en/latest/concepts/metrics/index.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "    \"Can I stop a SageMaker Autopilot job manually?\",\n",
    "    \"Do I get charged separately for each notebook created and run in SageMaker Studio?\",\n",
    "    \"Do I get charged for creating and setting up an SageMaker Studio domain?\",\n",
    "    \"Will my data be used or shared to update the base model that is offered to customers using SageMaker JumpStart?\",\n",
    "]\n",
    "\n",
    "#Defining the ground truth answers for each question\n",
    "EVAL_ANSWERS = [\n",
    "    \"Yes. You can stop a job at any time. When a SageMaker Autopilot job is stopped, all ongoing trials will be stopped and no new trial will be started.\",\n",
    "    \"\"\"No. You can create and run multiple notebooks on the same compute instance. \n",
    "    You pay only for the compute that you use, not for individual items. \n",
    "    You can read more about this in our metering guide.\n",
    "    In addition to the notebooks, you can also start and run terminals and interactive shells in SageMaker Studio, all on the same compute instance.\"\"\",\n",
    "    \"No, you donâ€™t get charged for creating or configuring an SageMaker Studio domain, including adding, updating, and deleting user profiles.\",\n",
    "    \"No. Your inference and training data will not be used nor shared to update or train the base model that SageMaker JumpStart surfaces to customers.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the batch invokation from Langchain to get answers for each question inside the `EVAL_QUESTIONS` list.\n",
    "\n",
    "Once we have the answer, RAGAS expect a dataset in a Hugging face format. Let's create the dataset before evaluating the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_dataset\n",
    "from ragas.metrics import answer_relevancy, faithfulness, answer_correctness\n",
    "from ragas import evaluate\n",
    "\n",
    "#Batch invoke and dataset creation\n",
    "result_batch_questions = chain.batch([{\"input\": q} for q in EVAL_QUESTIONS])\n",
    "\n",
    "dataset= build_dataset(EVAL_QUESTIONS,EVAL_ANSWERS,result_batch_questions, text_chunks)\n",
    "\n",
    "result = evaluate(dataset=dataset, metrics=[answer_relevancy, faithfulness, answer_correctness],llm=llm_bedrock, embeddings=bedrock_embeddings, raise_exceptions=False )\n",
    "df = result.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate RAG with LLamaIndex\n",
    "\n",
    "\n",
    "LlamaIndex is a data framework for LLM-based applications to ingest, structure, and access private or domain-specific data. Itâ€™s available in Python (these docs) and Typescript.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator\n",
    ")\n",
    "\n",
    "from utils import evaluate_llama_index_metric\n",
    "\n",
    "\n",
    "\n",
    "bedrock_llm_llama = Bedrock(model=BEDROCK_MODEL_ID)\n",
    "faithfulness= FaithfulnessEvaluator(llm=bedrock_llm_llama)\n",
    "answer_relevancy= AnswerRelevancyEvaluator(llm=bedrock_llm_llama)\n",
    "correctness= CorrectnessEvaluator(llm=bedrock_llm_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_faithfulness= evaluate_llama_index_metric(faithfulness, dataset)\n",
    "df_faithfulness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_answer_relevancy= evaluate_llama_index_metric(answer_relevancy, dataset)\n",
    "df_answer_relevancy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_correctness= evaluate_llama_index_metric(correctness, dataset)\n",
    "df_correctness.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
